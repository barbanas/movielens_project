## Machine Learning Models (Part II)


```{r load_common_objects_6, echo=FALSE}
  
 source("common.R")

```
In our third try we researched for the existence of Machine Learning models that are specialized in solving the problem set on this project.  This is similar to looking for specific ML models inside the caret package that can solve a problem you have, only this time you are looking everywhere (in the models included in the caret package and outside of it).

We set some minimum criteria for researching and selecting our next Machine Learning Model candidate.  According to these criteria, our next ML model candidate must:

+ be specialized on this kind of problem (recommender system algorithms) because each tool is designed for a specific task.  When you have a nail you use a hammer, not a screwdriver.

+ can handle easily (without crashing R Studio) and efficiently (without you to have to wait days or weeks for the train to be completed) the size of `r nrow(edx)` observations found inside the `edx` data set.  We should note here that in this project we were given a limited version (only 10 million ratings) of the original MovieLens data set which contains around 33 million ratings in total[^1].  If our algo fails to calculate a fit and make predictions on such large data sets, it will be impossible to be of any use to the original, full MovieLens data set or anything larger than that.

Our research pointed to various algorithms that are promising in achieving our set goal:

+ **Collaborative Filtering** (which are further divided into User-Based Collaborative Filtering and Item-Based Collaborative Filtering)

+ **Hybrid Recommender Systems** which are combining Collaborative Filtering and Content-Based Filtering methods together 

+ **Matrix Factorization Techniques** (which can use a Singular Value Decomposition (SVD) algorithm or an Alternating Least Squares (ALS) algorithm)

+ **Ensemble methods** that can combine the results of various algos into a voting system that possibly can reduce the RMSE even further

From the above we selected the **Matrix Factorization** Techniques to begin with, and see if we it we can achieve our goal with its help.  For that purpose we selected the **recommenderlab** package in R[^2] which is a "_Recommender System using Matrix Factorization_".  Recosystem is an R wrapper of the LIBMF library developed by Yu-Chin Juan, Wei-Sheng Chin, Yong Zhuang, Bo-Wen Yuan, Meng-Yuan Yang, and Chih-Jen Lin (https://www.csie.ntu.edu.tw/~cjlin/libmf/), an open source library for recommender system using parallel marix factorization (Chin, Yuan, et al. 2015).  This package offers a framework for developing and evaluating recommender systems. It contains various algorithms, evaluation metrics and utilities for building and assessing recommendation models. 

There is also additional reading material that one can read on this topic[^3].

[^1]: https://grouplens.org/datasets/movielns/
[^2]: https://cran.r-project.org/web/packages/recosystem/index.html and https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html
[^3]: "Building a Recommendation System with R" by Suresh K. Gorakala and "How to Code a Recommendation System in R" by Ander Fern√°ndez



### Training

In this attempt we will use the `recosystem` library. 

We start by loading the library and **converting** our data sets into the format that the `recosystem` library recognizes.  This is necessary in order to be able to train our model and then make predictions on the test data.

```{r recosystem_train_and_predict,echo=TRUE,warning=FALSE}

library(parallel) # used for parallel processing 
library(recosystem) # building, evaluating recommendation systems

library(kableExtra) # used for styling the kable tables

set.seed(20240424)

percentage_of_data_to_use <- 1 # how much data to take from the train set (1 = 100%)

file <- paste0(path_of_files, 
               "predictions/y_hat_rec_MF_with_",
               percentage_of_data_to_use,
               "_of_the_data.rds")
if(file.exists(file) == TRUE) {
# if the algo is already trained and its predictions already made, 
# load its predictions from the corresponding file 
    y_hat_rec_MF <- readRDS(file)   
    
} else {
  # set how many cores will be used in the parallel execution for training
  number_of_cores <- detectCores() - 2 
  
  # specify the indices that point to the user, movie and rating in the edx (train) 
  # data set. 
  train_set_reco <- with(edx, data_memory(user_index = userId, 
                                          item_index = movieId, 
                                          rating = rating))
  
  # do the same for the final_holdout_test (test) data set. 
  test_set_reco <- with(final_holdout_test, data_memory(user_index = userId, 
                                                        item_index = movieId, 
                                                        rating = rating))
  
  #
  # Note that we use the ENTIRE edx and final_holdout_test 
  # (not just a percentage of them)
  #
  
  # initialize a new object of a recommendation system
  recommendation_system <- Reco()
  
  # set the tuning parameters
  tuning_MF <- recommendation_system$tune(train_set_reco, 
                                          opts = list(dim = c(10, 20, 30, 40),
                                                      lrate = c(0.1, 0.2, 0.3, 0.4),
                                                      nthread  = number_of_cores,
                                                      niter = 20))
  
  # do the training with the train set
  recommendation_system$train(train_set_reco, 
                              opts = c(tuning_MF$min, 
                                       nthread = number_of_cores, 
                                       niter = 20)
                              )
  
  # do the predictions on the test set
  y_hat_rec_MF <- recommendation_system$predict(test_set_reco, out_memory())

  # save the results of the predictions
  saveRDS(y_hat_rec_MF, file = file)
  
}

```


### Predicting movie ratings

Once the training is finished, which was rather quick considering that we feed the **entire** data set, the `y_hat_rec_MF` now contains our predictions.  We use that to evaluate the RMSE of this model.

```{r recosystem_evaluation,echo=TRUE,warning=FALSE}

library(dplyr)

# calculate the RMSE of the predictions of our model
RMSE_of_recommender_system <- RMSE(final_holdout_test$rating, y_hat_rec_MF)

# save the results to this file for future use
saveRDS(RMSE_of_recommender_system, 
        file = paste0(path_of_files, 
                      "objects/RMSE_of_recommender_system.rds"))

# add the results to the comparison table
results_overview_table <- bind_rows(results_overview_table, 
                                    tibble(Model = 
                                             paste0("Matrix factorization",
                                               " (with 100% of the data)"),
                                           RMSE = RMSE_of_recommender_system))

```


### Conclusion

The result of our latest effort is showing on the last line of the table below:

```{r display_results,echo=TRUE,eval=TRUE}

# print the comparison table that contains all the models 
show_kable_table(results_overview_table)

```

This last method (Matrix Factorization) succeeded in beating the threshold of **`r minimum_RMSE`** by **`r minimum_RMSE -  RMSE_of_recommender_system`** (RMSE achieved = **`r RMSE_of_recommender_system `**). Thus our goal has been achieved.

It is worth mentioning that:

+ during the training of the model the end user is able to see a nice 0% to 100% progress bar that looks like this:

```{r previs_of_progress_bar,echo=TRUE,warning=FALSE,eval=FALSE}

  0%   10   20   30   40   50   60   70   80   90   100%
  [----|----|----|----|----|----|----|----|----|----|
  **

```

This bar gives you an idea of how long the whole process will take and how much it remains for the completion of the calculation.  This is very useful and we hope to see this also in the caret package.

+ the Matrix Factorization (MF) method achieved this result with the **full** data set (edx) with **all** `r nrow(edx) ` observations, not just a percentage of it.  Judging from the time needed for training this model as well as the way this algo was designed, we believe that this method can also handle larger data sets, by utilizing the computing power and the limiter RAM memory of an average home computer, without any problem.

