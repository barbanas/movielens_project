# Machine Learning (ML) Models


```{r load_common_objects_5, echo=FALSE,warning=FALSE}
  
 source("common.R")

```
In this Chapter we will try to see if we can use Machine Learning algos (from the caret package of R) to do the training and the prediction of movie ratings.

We first tried this with the full data set (the `edx` data set) on a 12 cores Intel Xeon machine with 40 GB of RAM available.  However, on all cases, the R Studio crashed due to insufficient memory.  One strange thing about R is that does not just report an error message like "Insufficient memory" and stop executing your code at the point where the error occurred (which is what is happening in all programming languages we know).  Instead when this error happens, the entire R Studio crashes.

So, it was necessary to have a **reduced** data set (both for training and for test) in order to just be able to run the ML algos and get some results.  

```{r echo=FALSE,warning=FALSE}

RMSE_of_gbm <- 0 # initialize this variable (we will use it later)

# how much data we will feed to our ML algos?
percentage_of_data_to_use <- 0.01  # any value above 0.10 crashes the R

```

We therefore selected a very small percentage (`r percentage_of_data_to_use` or `r percentage_of_data_to_use * 100`%) of the original data set in order to see if the algos will be trained like this without crashing R Studio. If this proves to be successful, we would be able to increase this percentage and get even better RMSE (we believe that the more data you have, the more accurate the prediction could be).

```{r show_libraries,echo=TRUE,warning=FALSE,eval=FALSE}

library(caret) # train and predict with ML algos
library(dplyr) # data manipulation

# libraries used for parallel execution by utilizing multiple cores of the CPU
library(doParallel)
library(parallel)

library(kableExtra) # used for styling the kable tables

```
```{r load_libraries,echo=FALSE,warning=FALSE}

suppressPackageStartupMessages(library(caret)) # train and predict with ML algos
suppressPackageStartupMessages(library(dplyr)) # data manipulation

# libraries used for parallel execution by utilizing multiple cores of the CPU
suppressPackageStartupMessages(library(doParallel))
suppressPackageStartupMessages(library(parallel))

suppressPackageStartupMessages(library(kableExtra)) # used for styling the kable tables

```
```{r echo=TRUE,warning=FALSE}

set.seed(20240423)

if (percentage_of_data_to_use == 1) {
  # use the entire data sets (100% of the data)
  train_set <- edx
  test_set <- final_holdout_test
  
} else {
# use a percentage_of_data_to_use of the data 
  train_set <- edx |> 
    slice(1:round(n() * percentage_of_data_to_use))
  
  test_set <- final_holdout_test |> 
    slice(1:round(n() * percentage_of_data_to_use))
  
}

```


## Machine Learning Models (Part I)


In our second attempt, we will use the very simple prediction formula:

+ **rating ~ userId + movieId**

which means "_predict the 'rating' a user may give to a movie by examining the predictors (variables) `userId` and `movieId` only_".

```{r prediction_formula_5,echo=TRUE}

# set what we want to predict and our predictors for the job
prediction_formula <- as.formula("rating ~ userId + movieId") 

```
```{r prediction_formula,echo=FALSE}
total_caret_models <- length(unique(modelLookup()$model))
```
As we learned in the course, there are many other Machine Learning models to choose from the caret package which, at the moment of writing this report, contains `r total_caret_models`(!) models.

To somehow limit the choices, we checked which algos are preferred for use in a movie recommendation system and we selected the following:

+ **Gradient Boosting Machines (GBM)**: a popular machine learning algorithm used for both, regression and classification.  Our MovieLens project falls under the "regression" category.

+ **Support Vector Regression (SVR)**: this algo is used for regression tasks.  SVR is available in caret through the svmRadial method.

+ **Random Forest**: an ensemble learning method used for both, regression and classification. This model proved to be the heavier in terms of RAM memory needed.

Since the above algos are part of the caret package we can use the `train()` function to train these models.

During the training we chose to use _cross validation_ (instead of splitting the `edx` data set into train and test set).

### Training

To increase the execution speed of our code, we will use Parallel processing.  Parallel processing is the execution of multiple tasks (or calculations), simultaneously.  This is done on by utilizing multiple CPU cores and it will allow the training of our model to take less time to be completed.

There are several ways to implement parallel processing in R. We selected the `doParallel` package for the job.

```{r training,echo=TRUE,warning=FALSE}

# Set how many cores will be used in the parallel execution for 
# train() and predict() below.  
# I leave 2 cores free for the basic operation of the OS itself
number_of_cores <- detectCores() - 2 

# # set this to TRUE if you want to use Tune Grids for the algos. 
# Note that TuneGrids increase the time a model needs for training
EnableTuneGrids <- TRUE 

cl <- makeCluster(number_of_cores) # create a parallel environment
registerDoParallel(cl)

# send to the parallel environment the data for training
clusterExport(cl, c("train_set"))  

  #
  # 1st ML algo
  #
  file <- paste0(path_of_files, 
                 "fits/fit_svmRadial_with_",
                 percentage_of_data_to_use,
                 "_of_the_data.rds")
  if(file.exists(file) == TRUE) {
  # if we have trained this model before, just load it from its file, 
  # otherwise train() it then save it to its file
      fit_svmRadial <- readRDS(file)    
    
  } else {
  # we have not trained this model before: do this now
      if (EnableTuneGrids == FALSE) {
        tg <- NULL
      } else {
        tg <- NULL
        #tg <- data.frame(# Number of neighbors: choose a range based on 
        #   data size and complexity
        #   k = seq(3, 50, 2)
        # )
      }
      
      # do the training
      fit_svmRadial <- train(prediction_formula,
                     data = train_set,
                     method = "svmRadial",
                     trControl = trainControl(allowParallel = TRUE, 
                                              # this runs the training in parallel
                                              verboseIter = TRUE, 
                                              method = "cv"), # use cross validation
                     tuneGrid = tg
      )
      
      # save the fit 
      saveRDS(fit_svmRadial, file = file)
      #rm(fit_svmRadial) # we do this in order to save memory; 
      # we can reload the object after ALL calculations have finished
      #gc() # clear free up memory (garbage collection)

  }

  #
  # 2nd ML algo
  #
  file <- paste0(path_of_files, 
                 "fits/fit_rf_with_",
                 percentage_of_data_to_use,
                 "_of_the_data.rds")
  if(file.exists(file) == TRUE) {
  # if we have trained this model before, just load it from its file, otherwise train() 
  # it then save it to its file
      fit_rf <- readRDS(file)    
      
  } else {    
  # we have not trained this model before: do this now
      if (EnableTuneGrids == FALSE) {
        tg <- NULL
        ns <- NULL
      } else {
        tg <- expand.grid(
          # Number of variables sampled at each split: try different ratios
          mtry = seq(2, sqrt(ncol(edx)))  # ncol is the number of predictor variables
          #splitrule = c("gini", "extratrees", "variance"),
          #ntree = c(100, 200, 500), # Number of trees: explore a range suitable 
          #  for data size and complexity
          #min.node.size = seq(1, 51, 10) # Minimum size of terminal nodes: prevent 
          # overfitting with large trees
        )
        ns <- seq(1, 51, 10)
      }
      
      # train
      fit_rf <- train(prediction_formula,
                      data = train_set,
                      method = "rf",
                      trControl = trainControl(allowParallel = TRUE, 
                                               verboseIter = TRUE, 
                                               method = "cv"),
                      tuneGrid = tg,
                      nodesize = ns
      )
      
      # save the results
      saveRDS(fit_rf, file = file)
      #rm(fit_rf) # we do this in order to save memory; we can
      # reload the object after ALL calculations have finished
      #gc() # clear free up memory (garbage collection)
      
  }

  #
  # 3rd ML algo
  #
  file <- paste0(path_of_files, "fits/fit_gbm_with_",
                 percentage_of_data_to_use,
                 "_of_the_data.rds")
  if(file.exists(file) == TRUE) {
      fit_gbm <- readRDS(file)  
      
  } else {    
    # train the model from scratch
      if (EnableTuneGrids == FALSE) {
        tg <- NULL
      } else {
        tg <- NULL #expand.grid(parameter = seq(0.01, 1, by = 0.01)) #), 
                   # family = "binomial"), # binomial for classification)
      }
    
      # train
      fit_gbm <- train(prediction_formula,
                     data = train_set,
                     method = "gbm",  #tried: xgboost, lightgbm but they are not part 
                                      # of the caret package
                     trControl = trainControl(allowParallel = TRUE, 
                                              verboseIter = TRUE, 
                                              method = "cv"),
                     tuneGrid = tg
      )
      
      # save the results
      saveRDS(fit_gbm, file = file)
      #rm(fit_gbm) # we do this in order to save memory; we can
      # reload the object after ALL calculations have finished
      #gc() # clear free up memory (garbage collection)
      
  }

# Stop the parallel backend
stopCluster(cl)


```

The training of all three models has been finished and we saved the fits into their respective files.  Doing so will allow us to load the fits directly without having to re-train the models.  Note that training is the most time (and energy) consuming task even with parallel processing that helped us a lot in finishing the calculations faster.


### Predicting movie ratings

Now, let's do some predictions on the `test_set` by using the three fits.

```{r predict_results,echo=TRUE}

# Make predictions
# Predict based on the Support Vector Machine (SVM) Radial model
file <- paste0(path_of_files, 
               "predictions/predictions_svmRadial_with_",
               percentage_of_data_to_use,
               "_of_the_data.rds") 
if(file.exists(file) == TRUE) {
# predictions have already been made. No need to do these again: just 
# load them from the file.
# We do (this to save time the 2nd, 3rd, etc time we run this script)
  predictions_svmRadial <- readRDS(file)
    
} else {
    # do the predictions with this model
    predictions_svmRadial <- predict(fit_svmRadial, 
                                     newdata = test_set) 
    
    # save the results
    saveRDS(predictions_svmRadial, file = file)
    
}

# Predict based on the Random Forest model
file <- paste0(path_of_files, 
               "predictions/predictions_rf_with_",
               percentage_of_data_to_use,
               "_of_the_data.rds") 
if(file.exists(file) == TRUE) {
# predictions have already been made. No need to do these again: just 
# load them from the file (this is to save time the 2nd, 3rd, etc 
# time you run this script).
    predictions_rf <- readRDS(file)
    
} else {
    # predict 
    predictions_rf <-predict(fit_rf, 
                             newdata = test_set) 
    
    # save the results
    saveRDS(predictions_rf, file = file)
    
}  

# Predict based on the Gradient Boosting Machine model
file <- paste0(path_of_files, 
               "predictions/predictions_gbm_with_",
               percentage_of_data_to_use,"
               _of_the_data.rds") 
if(file.exists(file) == TRUE) {
# predictions have already been made. No need to do these again: just load them 
# from the file (we do this to save time the 2nd, 3rd, etc time you run this script)
    predictions_gbm <- readRDS(file)
    
} else {
    # predict 
    predictions_gbm <- predict(fit_gbm, 
                               newdata = test_set) 
    
    # save the results
    saveRDS(predictions_gbm, file = file)
    
}

```

Three predictions were made based on each of our three models respectively.  

Below we will calculate the RMSE of each model and will summarize the results to our comparison table.

```{r summarize_results,echo=TRUE}

# read our results table
results_overview_table <- readRDS(paste0(path_of_files, 
                                         "results/results_overview_table.rds")) 

# add the RMSE of the prediction results into the comparison table
# add the results of the SVM model
results_overview_table <- bind_rows(results_overview_table, 
                                    tibble(Model = 
                                             paste0("Support Vector Machine (SVM) ",
                                                    "Radial (with ", 
                                                    percentage_of_data_to_use, 
                                                    " of the data)"),
                                           RMSE = 
                                             RMSE(test_set$rating, 
                                                  predictions_svmRadial)))

# add the results of the RF model
results_overview_table <- bind_rows(results_overview_table, 
                                    tibble(Model = paste0("Random Forest (with ",
                                                          percentage_of_data_to_use,
                                                          " of the data)"),
                                           RMSE = RMSE(test_set$rating, 
                                                       predictions_rf)))

# add the results of GBM model
results_overview_table <- bind_rows(results_overview_table, 
                                    tibble(Model =paste0("Gradient Boosting Machine ",
                                                           "(with ",
                                                           percentage_of_data_to_use,
                                                           " of the data)"),
                                           RMSE = RMSE(test_set$rating, 
                                                       predictions_gbm)))

# the GBM model seems that it estimated with greater accuracy. Save its 
# RMSE for future use
RMSE_of_gbm <- RMSE(test_set$rating, predictions_gbm)  
saveRDS(RMSE_of_gbm, file = paste0(path_of_files, "objects/RMSE_of_gbm.rds"))

```

The predictions are finished in reasonable time.  In general, predictions are faster than the training of a model, especially if we have a lot of data.

### Conclusion

Using only `r percentage_of_data_to_use` of the `edx` (train set) and the `final_holdout_test` (test set), the last three machine learning models that we trained (via the caret package), failed to beat either the threshold of `r minimum_RMSE` or the "Mean rating + movie bias + user bias" model.  The results are showing on the three last lines of the table below:

```{r show_results,echo=TRUE,results='Comparison Table I'}

# print the results so far
show_kable_table(results_overview_table)

```

From the results table it is obvious that:

+ All machine learning models produced an RMSE which is _above_ the threshold of **`r minimum_RMSE`**, which means that failed to accomplish our goal.  Even the **Gradient Boosting Machine** model that performed better that any other ML model, had an RMSE = **`r RMSE_of_gbm `**.  Our experiments showed that more data will yield better (more accurate) results because a larger part of the reality is captured on the additional data and therefore the prediction will inevitably be more accurate.

+ The data that were used for training and test were an extremely small sample (`r percentage_of_data_to_use`) of the data sets `edx` and `final_holdout_test`. This percentage was selected in order to allow us see what will be the cost in time and energy for training our Machine Learning models with just a fraction of the entire data set.  And although the percentage of data to use was extremely small, the time that required to train these three models easily exceeded the 24Hours on a machine with technical specifications (on CPU, GPU and RAM memory) above the technical specifications of an average home computer today. This leads to the conclusion that any percentage of data above `r percentage_of_data_to_use` and surely for the 100% of the data, will require computing capabilities that are comparable to those of a GPU Server.

+ It is evident that the successful completion of the goal of this project with a Machine Learning model, requires a different plan of attack (a different approach) since access to (or ownership of) a GPU Server is not common (a GPU Server may costs today around 250.000 USD depending on the configuration selected).

On the next Chapter we will explore a Machine Learning algorithm that is designed especially for this kind of problem and it works efficiently with large amounts of data like the data on hand.

<div style="page-break-before: always;"></div>